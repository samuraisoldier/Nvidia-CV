{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through how to create your own dataset and use it to train a machine learning model to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's import the tools that we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set up acceleration using a GPU if we can (don't worry about this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda=False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset\n",
    "\n",
    "Here we will create a simple dataset \n",
    "\n",
    "We will create a folder called 'data'\n",
    "\n",
    "Inside this we will make folders that contain images of different classes, and name thos sub folders after the class of image that they contain.\n",
    "\n",
    "E.g. we could put folders full of images of cats and dogs inside the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv(root=data_root, out_name='labels.csv'):\n",
    "    \"\"\"This function finds images in each of the sub folders and creates a csv file that tells us which class each image belongs to\"\"\"\n",
    "    subfolders = [f.path for f in os.scandir(root) if f.is_dir()]\n",
    "    df = pd.DataFrame(columns=['file_path', 'label'])\n",
    "    for i, path in enumerate(subfolders):\n",
    "        files = [f.path for f in os.scandir(path) if f.is_file()]\n",
    "        for f in files:\n",
    "            df = df.append({'file_path':f, 'label':i}, ignore_index=True)\n",
    "    df.to_csv(root+out_name, index=False)\n",
    "    \n",
    "create_csv()\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv=data_root + 'labels.csv', transform=None):\n",
    "        \"\"\"\"\"\"\n",
    "        self.csv = pd.read_csv(csv)\n",
    "        self.data_size = len(self.csv)\n",
    "        self.idx_to_data = dict(zip(range(self.data_size), zip(self.csv['file_path'].tolist(), self.csv['label'].tolist())))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, label = self.idx_to_data[int(idx)]\n",
    "        img = Image.open(filepath)\n",
    "        if self.transform:\n",
    "            img, label = self.transform((img, label))\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing our data\n",
    "\n",
    "The model we are going to create to map our inputs to our outputs has a fixed input size, so we need to resize any images that we will pass through it.\n",
    "\n",
    "We need to make some transforms that will be called on each input, to prepare it for a forward pass through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SquareResize():\n",
    "    \"\"\"Adjust aspect ratio of image to make it square\"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple)) # assert output_size is int or tuple\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):        \n",
    "        image, label = sample\n",
    "        h, w = image.size\n",
    "        if h>w:\n",
    "            new_w = self.output_size\n",
    "            scale = new_w/w\n",
    "            new_h = scale*h\n",
    "        elif w>h:\n",
    "            new_h = self.output_size\n",
    "            scale = new_h/h\n",
    "            new_w = scale*w\n",
    "        else:\n",
    "            new_h, new_w = self.output_size, self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w) # account for non-integer computed dimensions (rounds to nearest int)\n",
    "        image = image.resize((new_h, new_w))\n",
    "        image = image.crop((0, 0, self.output_size, self.output_size))\n",
    "        return image, label\n",
    "\n",
    "class ToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        image = np.array(image)/255\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return torch.Tensor(image), label\n",
    "    \n",
    "ytransforms = []\n",
    "mytransforms.append(SquareResize(224))\n",
    "mytransforms.append(ToTensor())\n",
    "mytransforms = transforms.Compose(mytransforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dataset and the transforms to preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydataset = ClassificationDataset(csv=data_root + 'labels.csv', transform=mytransforms)\n",
    "\n",
    "\n",
    "data_size=len(mydataset)\n",
    "train_size = int(train_split * data_size)\n",
    "val_size = int(val_split * data_size) - train_size\n",
    "test_size = data_size - (val_size + train_size)\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(mydataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed these datasets to our models, we can create a dataloader.\n",
    "\n",
    "A dataloader loads our examples from the datasets for us, and does some useful things for us including batching together input examples and shuffling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model to map inputs to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VGGClassifier(torch.nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.features = models.vgg11(pretrained=True).features #512x7x7\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512*7*7, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, out_size),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"This function is called on the data for the forward pass\"\"\"\n",
    "        x = F.relu(self.features(x)).reshape(-1, 512*7*7)\n",
    "        x = self.regressor(x)\n",
    "        return x     # return the output of the model\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the final function to train, and also visualise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs):\n",
    "    plt.close()\n",
    "    mymodel.train()\n",
    "    \n",
    "    bcosts = []\n",
    "    ecosts = []\n",
    "    valcosts = []\n",
    "    plt.ion()\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(121)\n",
    "    #ax1 = fig.add_subplot(132)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    plt.show()\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Cost')\n",
    "\n",
    "    #ax1.set_xlabel('Batch')\n",
    "    #ax1.set_ylabel('Cost')\n",
    "\n",
    "    ax2.axis('off')\n",
    "    img_label_text = ax2.text(0, -5, '', fontsize=15)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        ecost=0\n",
    "        valcost=0\n",
    "        for i, (x, y) in enumerate(train_samples):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x[:, :3, :, :]\n",
    "            print(x.shape)\n",
    "\n",
    "            h = mymodel.forward(x) #calculate hypothesis\n",
    "            cost = F.cross_entropy(h, y, reduction='sum') #calculate cost\n",
    "            \n",
    "            optimizer.zero_grad() #zero gradients\n",
    "            cost.backward() # calculate derivatives of values of filters\n",
    "            optimizer.step() #update parameters\n",
    "\n",
    "            bcosts.append(cost.item()/batch_size)\n",
    "            #ax1.plot(bcosts, 'b', label='Train cost')\n",
    "            #if e==0 and i==0: ax1.legend()\n",
    "            \n",
    "            y_ind=0\n",
    "            im = np.array(x[y_ind]).transpose(1, 2, 0)\n",
    "            predicted_class = id_to_classname[h.max(1)[1][y_ind].item()]\n",
    "            ax2.imshow(im)\n",
    "            img_label_text.set_text('Predicted class: '+ predicted_class)\n",
    "            \n",
    "            fig.canvas.draw()\n",
    "            ecost+=cost.item()\n",
    "        #classes_shown=set()\n",
    "        \"\"\"\n",
    "        for i, (x, y) in enumerate(val_samples):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            h = mymodel.forward(x) #calculate hypothesis\n",
    "            cost = F.cross_entropy(h, y, reduction='sum') #calculate cost\n",
    "\n",
    "            '''for y_ind, yval in enumerate(y):\n",
    "                if yval.item() not in classes_shown:\n",
    "                    classes_shown.add(yval.item())\n",
    "                    break'''\n",
    "            y_ind=0\n",
    "            im = np.array(x[y_ind]).transpose(1, 2, 0)\n",
    "            predicted_class = id_to_classname[h.max(1)[1][y_ind].item()]\n",
    "            ax2.imshow(im)\n",
    "            img_label_text.set_text('Predicted class: '+ predicted_class)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            valcost+=cost.item()\n",
    "            \"\"\"\n",
    "        ecost/=train_size\n",
    "        #valcost/=val_size\n",
    "        ecosts.append(ecost)\n",
    "        #valcosts.append(valcost)\n",
    "        ax.plot(ecosts, 'b', label='Train cost')\n",
    "        #ax.plot(valcosts, 'r', label='Validation cost')\n",
    "        if e==0: ax.legend()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        print('Epoch', e, '\\tCost', ecost)\n",
    "        \n",
    "\n",
    "def test():\n",
    "    print('Started evaluation...')\n",
    "    mymodel.eval() #put model into evaluation mode\n",
    "    \n",
    "    #calculate the accuracy of our model over the whole test set in batches\n",
    "    correct = 0\n",
    "    for x, y in test_samples:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        h = mymodel.forward(x)\n",
    "        pred = h.data.max(1)[1]\n",
    "        correct += pred.eq(y).sum().item()\n",
    "    return round(correct/len(test_data), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets actually do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mymodel.freeze()\n",
    "train(20)\n",
    "#mymodel.unfreeze()\n",
    "#train(5)\n",
    "\n",
    "acc = test()\n",
    "print('Test accuracy: ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
